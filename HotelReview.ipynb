{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a6958d5",
   "metadata": {},
   "source": [
    "## Hotel Review\n",
    "\n",
    "We aim to perform sentiment analysis on customer reviews to understand their \n",
    "sentiments towards a product or service. This helps us to understand customer \n",
    "satisfaction and areas that need improvement.\n",
    " \n",
    "NLP Techniques: \n",
    "1. Text Preprocessing: Tokenization, stop word removal and lemmatization. \n",
    "2. Feature Extraction: Bag of Words, Term Frequency â€“ Inverse Document \n",
    "Frequency (TF-IDF) and word embeddings. \n",
    "3. Sentiment Classification: Random Forest (RF) Classifier, Logistic Regression, \n",
    "CNN "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f193a2",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "Each customer review is a textual feedback and an overall rating.  \n",
    "The ratings can range from 1 to 10.  \n",
    "We will split them into two categories: bad reviews have ratings < 5 and good reviews have ratings >= 5.\n",
    "\n",
    "The textual is divide into two part (positive and negative). We group the together in order to start with only one raw text data.\n",
    "additionally if the user doesn't leave any negative or positive comment, this will appear as \"No Negative\" or \"No Positive\". those part have to be removed from the text."
   ]
  },
  {
   "cell_type": "code",
   "id": "c92af576",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T12:46:28.754387Z",
     "start_time": "2024-06-24T12:46:28.361326Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "\n",
    "#read data\n",
    "reviews = pd.read_pickle('dataset/Hotel_Reviews.pkl')\n",
    "\n",
    "#append the positive and negative reviews\n",
    "reviews['review'] = reviews['Negative_Review'] + reviews['Positive_Review']\n",
    "#create the label\n",
    "reviews['review_type'] = reviews['Reviewer_Score'].apply(lambda x: 'Bad_review' if x < 5 else 'Good_review')\n",
    "#sample data in order to speed up the computation\n",
    "reviews = reviews.sample(frac=0.1, replace=False, random_state=42)\n",
    "#clean data\n",
    "reviews['review'] = reviews['review'].apply(lambda x: x.replace('No Negative', '').replace('No Positive', '')) \n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "bc0005d2",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "\n",
    "We will perform several transformations to clean the textual data:\n",
    "- tokenize the text and remove the punctuation\n",
    "- remove useless stop words\n",
    "- lemmatize the text"
   ]
  },
  {
   "cell_type": "code",
   "id": "838f05a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T12:46:30.283452Z",
     "start_time": "2024-06-24T12:46:28.755390Z"
    }
   },
   "source": [
    "#lemmatize token and remove stop word, if len of word is greater than 1 remove it \n",
    "from typing import List\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "def clean_text(text: str) -> List[str]:\n",
    "    doc = nlp(text)\n",
    "    return [token.lemma_ for token in doc if not token.is_stop and len(token) > 1]\n",
    "\n",
    "reviews['review_clean'] = reviews['review'].apply(lambda x: clean_text(x))"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "issubclass() arg 1 must be a class",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-2-c7436fdc8a58>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[1;31m#lemmatize token and remove stop word, if len of word is greater than 1 remove it\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtyping\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mList\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 3\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mspacy\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      4\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[0mnlp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mspacy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mload\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'en_core_web_sm'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdisable\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'parser'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'ner'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mthinc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mapi\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mConfig\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 14\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpipeline\u001B[0m  \u001B[1;31m# noqa: F401\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     15\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mcli\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0minfo\u001B[0m  \u001B[1;31m# noqa: F401\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mglossary\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mexplain\u001B[0m  \u001B[1;31m# noqa: F401\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mattributeruler\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mAttributeRuler\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mdep_parser\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDependencyParser\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0medit_tree_lemmatizer\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mEditTreeLemmatizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mentity_linker\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mEntityLinker\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mner\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mEntityRecognizer\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\attributeruler.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mpathlib\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPath\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mpipe\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mPipe\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0merrors\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mErrors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtraining\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mExample\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\pipeline\\pipe.pyx\u001B[0m in \u001B[0;36minit spacy.pipeline.pipe\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\vocab.pyx\u001B[0m in \u001B[0;36minit spacy.vocab\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mdoc\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDoc\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mtoken\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mToken\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mspan\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSpan\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0mspan_group\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mSpanGroup\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[1;33m.\u001B[0m\u001B[0m_serialize\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mDocBin\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\tokens\\doc.pyx\u001B[0m in \u001B[0;36minit spacy.tokens.doc\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\schemas.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    214\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    215\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 216\u001B[1;33m \u001B[1;32mclass\u001B[0m \u001B[0mTokenPattern\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mBaseModel\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    217\u001B[0m     \u001B[0morth\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mStringValue\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    218\u001B[0m     \u001B[0mtext\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mOptional\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mStringValue\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\main.py\u001B[0m in \u001B[0;36m__new__\u001B[1;34m(mcs, name, bases, namespace, **kwargs)\u001B[0m\n\u001B[0;32m    297\u001B[0m                     ):\n\u001B[0;32m    298\u001B[0m                         \u001B[1;32mcontinue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 299\u001B[1;33m                     fields[ann_name] = ModelField.infer(\n\u001B[0m\u001B[0;32m    300\u001B[0m                         \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mann_name\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    301\u001B[0m                         \u001B[0mvalue\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mvalue\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\fields.py\u001B[0m in \u001B[0;36minfer\u001B[1;34m(cls, name, value, annotation, class_validators, config)\u001B[0m\n\u001B[0;32m    409\u001B[0m             \u001B[0mrequired\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    410\u001B[0m         \u001B[0mannotation\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_annotation_from_field_info\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mannotation\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mfield_info\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mvalidate_assignment\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 411\u001B[1;33m         return cls(\n\u001B[0m\u001B[0;32m    412\u001B[0m             \u001B[0mname\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    413\u001B[0m             \u001B[0mtype_\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mannotation\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\fields.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, name, type_, class_validators, model_config, default, default_factory, required, alias, field_info)\u001B[0m\n\u001B[0;32m    340\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mint\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mSHAPE_SINGLETON\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    341\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_config\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprepare_field\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 342\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprepare\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    343\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    344\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mget_default\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mAny\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\fields.py\u001B[0m in \u001B[0;36mprepare\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    449\u001B[0m             \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    450\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 451\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_type_analysis\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    452\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequired\u001B[0m \u001B[1;32mis\u001B[0m \u001B[0mUndefined\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    453\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrequired\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\fields.py\u001B[0m in \u001B[0;36m_type_analysis\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    543\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mouter_type_\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtype_\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    544\u001B[0m                 \u001B[1;31m# re-run to correctly interpret the new self.type_\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 545\u001B[1;33m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_type_analysis\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    546\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    547\u001B[0m                 \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msub_fields\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_create_sub_type\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mt\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34mf'{self.name}_{display_as_type(t)}'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mtypes_\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\site-packages\\pydantic\\fields.py\u001B[0m in \u001B[0;36m_type_analysis\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    548\u001B[0m             \u001B[1;32mreturn\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    549\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 550\u001B[1;33m         \u001B[1;32mif\u001B[0m \u001B[0missubclass\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0morigin\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mTuple\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m  \u001B[1;31m# type: ignore\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    551\u001B[0m             \u001B[1;31m# origin == Tuple without item type\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    552\u001B[0m             \u001B[0margs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mget_args\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtype_\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\anaconda3\\lib\\typing.py\u001B[0m in \u001B[0;36m__subclasscheck__\u001B[1;34m(self, cls)\u001B[0m\n\u001B[0;32m    772\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_special\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    773\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_GenericAlias\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 774\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0missubclass\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__origin__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    775\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_special\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    776\u001B[0m                 \u001B[1;32mreturn\u001B[0m \u001B[0missubclass\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mcls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__origin__\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__origin__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mTypeError\u001B[0m: issubclass() arg 1 must be a class"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "8fd3ddd0",
   "metadata": {},
   "source": [
    "### tfidf Encoding\n",
    "\n",
    "We want to create document embeddings using bag-of-words approach "
   ]
  },
  {
   "cell_type": "code",
   "id": "a25d5e93",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T12:46:30.285983Z",
     "start_time": "2024-06-24T12:46:30.284677Z"
    }
   },
   "source": [
    "from typing import List\n",
    "from scipy.sparse import lil_matrix\n",
    "import numpy as np\n",
    "\n",
    "class TfIdfModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.index = dict()\n",
    "        self.precomputed_embeddings = dict()\n",
    "        \n",
    "    #Create an index for the vocabulary from the docs\n",
    "\n",
    "    def build_index(self, docs: List[List[str]]) -> None:\n",
    "        words = [word for doc in docs for word in doc]\n",
    "        self.index = {word:i for i, word in enumerate(sorted(set(words)))}\n",
    "\n",
    "\n",
    "    def train(self, docs: List[List[str]]) -> None:\n",
    "        self.build_index(docs)\n",
    "        num_docs = len(docs)\n",
    "        num_terms = len(self.index)\n",
    "    \n",
    "        # Use a sparse matrix\n",
    "        term_doc_matrix = lil_matrix((num_docs, num_terms), dtype=np.float64)\n",
    "    \n",
    "        # Compute the term frequency matrix\n",
    "        for i, doc in enumerate(docs):\n",
    "            for term in doc:\n",
    "                term_doc_matrix[i, self.index[term]] += 1\n",
    "    \n",
    "        # Convert to CSR format for efficient arithmetic operations\n",
    "        term_doc_matrix = term_doc_matrix.tocsr()\n",
    "    \n",
    "        td_log_matrix = term_doc_matrix.copy()\n",
    "        td_log_matrix.data = np.log10(td_log_matrix.data + 1)\n",
    "    \n",
    "        df_vector = np.diff(term_doc_matrix.tocsc().indptr)\n",
    "        df_vector[df_vector == 0] = 1\n",
    "        idf_vector = np.log10(num_docs / df_vector)\n",
    "    \n",
    "        self.tfidf_matrix = td_log_matrix.multiply(idf_vector)\n",
    "\n",
    "        for term, idx in self.index.items():\n",
    "            self.precomputed_embeddings[term] = self.tfidf_matrix.getcol(idx).toarray().flatten()\n",
    "\n",
    "    def embed(self, word: str) -> np.ndarray:\n",
    "        return self.precomputed_embeddings.get(word, None)\n",
    "\n",
    "    def vector_size(self) -> int:\n",
    "        return self.tfidf_matrix.shape[0]\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Bag of Words\n",
    "Create a document embedding using the bag of words approach"
   ],
   "id": "dc4c63bb3d066312"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def bagOfWords(model: TfIdfModel, doc: List[str]) -> np.ndarray:\n",
    "    embeds = [model.embed(token) for token in doc if token in model.precomputed_embeddings]\n",
    "    \n",
    "    if embeds:\n",
    "        embeds = np.array(embeds)\n",
    "        return embeds.mean(axis=0)\n",
    "    \n",
    "    return np.zeros(model.vector_size())\n"
   ],
   "id": "13b1a288",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aae37be3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-24T12:46:30.287986Z",
     "start_time": "2024-06-24T12:46:30.287986Z"
    }
   },
   "source": [
    "model = TfIdfModel()  \n",
    "model.train(reviews['review_clean'].tolist())\n",
    "\n",
    "embed_train = np.array([bagOfWords(model, review) for review in reviews['review_clean']])\n",
    "labels_train = np.array([round(score) for score in reviews['Reviewer_Score']])\n",
    "\n",
    "print(embed_train.shape)\n",
    "print(labels_train.shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### plot the confusion matrix and calculate f1_score",
   "id": "7d5296331ecce57b"
  },
  {
   "cell_type": "code",
   "id": "d0add960",
   "metadata": {},
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "\n",
    "# sample data\n",
    "np.random.seed(0)\n",
    "embed_train = np.random.rand(20000, 20000)\n",
    "labels_train = np.random.randint(0, 1000, 20000)\n",
    "\n",
    "# Dimensionality reduction using PCA\n",
    "pca = PCA(n_components=200)\n",
    "embed_train_reduced = pca.fit_transform(embed_train)\n",
    "\n",
    "unique_classes = np.unique(labels_train)[:10]\n",
    "mask = np.isin(labels_train, unique_classes)\n",
    "\n",
    "embed_train_subset = embed_train_reduced[mask]\n",
    "labels_train_subset = labels_train[mask]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "labels_train_subset_encoded = label_encoder.fit_transform(labels_train_subset)\n",
    "\n",
    "classifier = SVC(kernel='poly')\n",
    "classifier.fit(embed_train_subset, labels_train_subset_encoded)\n",
    "\n",
    "train_predict_subset = classifier.predict(embed_train_subset)\n",
    "cm = confusion_matrix(labels_train_subset_encoded, train_predict_subset, normalize='true')\n",
    "\n",
    "# Use a heatmap with a logarithmic scale to better visualize the data\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(np.log1p(cm), annot=True, fmt=\".2f\", cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
    "plt.title('Confusion Matrix - Training Set (Subset)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "f1_score_train_subset = f1_score(labels_train_subset_encoded, train_predict_subset, average='micro')\n",
    "print(\"F1 Score - Training Set (Subset):\", f1_score_train_subset)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63d3572d",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
